<!doctype html><html lang=en><head><title>Concrete AI safety problems · wlsc</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Wladimir Schmidt"><meta name=description content="The &ldquo;Concrete AI safety problems&rdquo; paper by Dario Amodei (Google Brain), Chris Olah (Google Brain), Jacob
Steinhardt (Stanford University), Paul Christiano (UC Berkeley), John Schulman (OpenAI), Dan Mané (Google Brain)
suggests a new approach to the Machine Learning (ML) and Artificial Intelligence (AI) research, which focuses more on
productivity of forward-looking applications while building cutting-edge AI systems.
A number of key problems considered in the paper as well as their descriptions are listed below. In the paper authors
also elaborate on how to approach each of the given problem."><meta name=keywords content="Wladimir Schmidt,Principal Software Engineer,Principal Software Developer,Distributed Systems Engineer,Stream Processing,Real-Time Processing,Big Data,Research and Development,R&amp;D,Expert Developer,Development Architect,personal blog,wlsc,software,engineering,business software,Cloud-Native Applications,machine learning,thoughts,resume,cv"><meta name=twitter:card content="summary"><meta name=twitter:title content="Concrete AI safety problems"><meta name=twitter:description content="The “Concrete AI safety problems” paper by Dario Amodei (Google Brain), Chris Olah (Google Brain), Jacob Steinhardt (Stanford University), Paul Christiano (UC Berkeley), John Schulman (OpenAI), Dan Mané (Google Brain) suggests a new approach to the Machine Learning (ML) and Artificial Intelligence (AI) research, which focuses more on productivity of forward-looking applications while building cutting-edge AI systems.
A number of key problems considered in the paper as well as their descriptions are listed below. In the paper authors also elaborate on how to approach each of the given problem."><meta property="og:url" content="https://wlsc.de/2016/06/22/concrete-ai-safety-problems/"><meta property="og:site_name" content="wlsc"><meta property="og:title" content="Concrete AI safety problems"><meta property="og:description" content="The “Concrete AI safety problems” paper by Dario Amodei (Google Brain), Chris Olah (Google Brain), Jacob Steinhardt (Stanford University), Paul Christiano (UC Berkeley), John Schulman (OpenAI), Dan Mané (Google Brain) suggests a new approach to the Machine Learning (ML) and Artificial Intelligence (AI) research, which focuses more on productivity of forward-looking applications while building cutting-edge AI systems.
A number of key problems considered in the paper as well as their descriptions are listed below. In the paper authors also elaborate on how to approach each of the given problem."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-06-22T11:20:06+00:00"><meta property="article:modified_time" content="2016-06-22T11:20:06+00:00"><meta property="article:tag" content="Paper"><meta property="article:tag" content="Safety"><link rel=canonical href=https://wlsc.de/2016/06/22/concrete-ai-safety-problems/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.ebdf1a5dca6a69142e979b32668c69f2a95448b145a168104c5808b14d2b75b0.css integrity="sha256-698aXcpqaRQul5syZoxp8qlUSLFFoWgQTFgIsU0rdbA=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://wlsc.de/>wlsc
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Tech.blog</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://wlsc.de/2016/06/22/concrete-ai-safety-problems/>Concrete AI safety problems</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2016-06-22T11:20:06Z>June 22, 2016
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
2-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/artificial-intelligence/>Artificial Intelligence</a>
<span class=separator>•</span>
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/paper/>Paper</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/safety/>Safety</a></span></div></div></header><div class=post-content><p>The <strong>&ldquo;Concrete AI safety problems&rdquo;</strong> paper by <em>Dario Amodei (Google Brain), Chris Olah (Google Brain), Jacob
Steinhardt (Stanford University), Paul Christiano (UC Berkeley), John Schulman (OpenAI), Dan Mané (Google Brain)</em>
suggests a new approach to the Machine Learning (ML) and Artificial Intelligence (AI) research, which focuses more on
productivity of forward-looking applications while building cutting-edge AI systems.</p><p>A number of key problems considered in the paper as well as their descriptions are listed below. In the paper authors
also elaborate on how to approach each of the given problem.</p><h2 id=key-ai-safety-problems>Key AI Safety Problems
<a class=heading-link href=#key-ai-safety-problems><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p><strong>Avoiding Negative Side Effects:</strong> How can we ensure that our cleaning robot will not disturb the environment in
negative ways while pursuing its goals, e.g. by knocking over a vase because it can clean faster by doing so? Can we
do this without manually specifying everything the robot should not disturb?</p></li><li><p><strong>Avoiding Reward Hacking:</strong> How can we ensure that the cleaning robot won&rsquo;t game its reward function? For example, if
we reward the robot for achieving an environment free of messes, it might disable its vision so that it won&rsquo;t find any
messes, or cover over messes with materials it can&rsquo;t see through, or simply hide when humans are around so they can&rsquo;t
tell it about new types of messes.</p></li><li><p><strong>Scalable Oversight:</strong> How can we efficiently ensure that the cleaning robot respects aspects of the objective that
are too expensive to be frequently evaluated during training? For instance, it should throw out things that are
unlikely to belong to anyone, but put aside things that might belong to someone (it should handle stray candy wrappers
differently from stray cellphones). Asking the humans involved whether they lost anything can serve as a check on
this, but this check might have to be relatively infrequent – can the robot find a way to do the right thing despite
limited information?</p></li><li><p><strong>Safe Exploration:</strong> How do we ensure that the cleaning robot doesn&rsquo;t make exploratory moves with very bad
repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an
electrical outlet is a very bad idea.</p></li><li><p><strong>Robustness to Distributional Shift:</strong> How do we ensure that the cleaning robot recognizes, and behaves robustly,
when in an environment different from its training environment? For example, heuristics it learned for cleaning
factory workfloors may be outright dangerous in an office.</p></li></ul><h2 id=reference>Reference
<a class=heading-link href=#reference><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><em>Source: <a href=https://arxiv.org/abs/1606.06565 class=external-link target=_blank rel=noopener>Concrete Problems in AI Safety</a></em></p></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2025
Wladimir Schmidt
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=GTM-W6RQWQXZ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","GTM-W6RQWQXZ")</script></body></html>